# M√ºller Alexader
# \033[0;__;__m \033[m   or   \033[1;__;__m \033[m
# print('#' * 115)      # –î–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –±–ª–æ–∫–æ–≤ –Ω–∞ –ª–∏—Å—Ç–µ —Å –∫–æ–¥–æ–º:
""" ################################################################################################################
 31.05.25
 AI  &  Python HW_10: 10. –†–∞–±–æ—Ç–∞ —Å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏: –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∑–≤—É–∫, –≤–∏–¥–µ–æ.
 ################################################################################################################### """

# ------------------------ SHORTCUTS ---------------------------------------
# Ctrl + W - –≤—ã–¥–µ–ª–∏—Ç—å —Ç–µ–∫—É—â–∏–π –±–ª–æ–∫. –µ—Å–ª–∏ –Ω–∞–∂–∏–º–∞—Ç—å —ç—Ç–æ —Å–æ—á–µ—Ç–∞–Ω–∏–µ –¥–∞–ª—å—à–µ, —Ç–æ –±—É–¥—É—Ç –≤—ã–¥–µ–ª—è—Ç—å—Å—è —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–µ –±–ª–æ–∫–∏.
# Ctrl+Y - –£–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ–π —Å—Ç—Ä–æ–∫–∏. –ö—Å—Ç–∞—Ç–∏, –∫–æ–º–∞–Ω–¥–∞ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è Ctrl+C –±–µ–∑ –≤—ã–¥–µ–ª–µ–Ω–∏—è —Ç–∞–∫–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –≤—Å–µ–π —Å—Ç—Ä–æ–∫–∏.
# Ctrl+Akt+L / Alt+Enter - –ü—Ä–∏–≤–µ—Å—Ç–∏ –∫–æ–¥ –∫ –ø—Ä–∏–Ω—è—Ç—ã–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º (–¥–ª—è Python - PEP8).
# Ctrl+R ‚Äî –ò–∑–º–µ–Ω–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞/—Ñ—É–Ω–∫—Ü–∏–∏ –∏ —Ç. –ø. –ø–æ –≤—Å–µ–º—É –ø—Ä–æ–µ–∫—Ç—É.
# Ctrl+Shift + F - –ù–∞–π—Ç–∏ –ø–æ –≤—Å–µ–º —Ñ–∞–π–ª–∞–º.
# Shift + F6 - –∑–∞–º–µ–Ω–∏—Ç—å –∏–º—è —ç–ª–µ–º–µ–Ω—Ç–∞ –≤–æ –≤—Å–µ—Ö —á–∞—Å—Ç—è—Ö –≤–æ –≤—Å–µ—Ö —Ñ–∞–π–ª–∞—Ö.

# --------------------------------------------------------------------------

l = 60      # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã —Å—Ç—Ä–æ–∫–∏ –Ω–∞ —ç–∫—Ä–∞–Ω–µ.
print('.' * l)

""" ___  NB!  ___   |==>  """
#   1)  See Homeworks/HW_04/hw04_23_05-Queries_Embed.py
#   2)  Video 10, 28:10. link: https://player.vimeo.com/video/1089059944?h=7f52960ab7
#   3)  fine_tuning_gpt_Stas_2.py


""" %%%%%%%%   Task 1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% """

# \\\\\\\\     –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —á–µ–ª–æ–≤–µ–∫ –Ω–µ–≤—Ä–æ—Ç–∏–∫–æ–º     ///////////////////////////////////////////////
# –í –æ–±—É—á–∞—é—â–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ —É—Ä–æ–≤–Ω–∏ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞:
#   4 - —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–µ –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è –Ω–µ–≤—Ä–æ—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—Ç—Ä–æ–π—Å—Ç–≤–∞.
#   0 - —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—é –Ω–µ–≤—Ä–æ—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—Ç—Ä–æ–π—Å—Ç–≤–∞, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ–≥–æ –≤ —Å–µ–±–µ —á–µ–ª–æ–≤–µ–∫–∞ —Å
#       –∞–¥–µ–∫–≤–∞—Ç–Ω—ã–º–∏ –ª–∏—á–Ω—ã–º–∏ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏, –ª—é–±—è—â–µ–≥–æ —Å–µ–±—è.

# ___ –ü–û–ó–ñ–ï –ü–û–ß–ò–¢–ê–¢–¨ —ç—Ç—É —Å—Ç–∞—Ç—å—é  ___
# Lin Zhou, Wenjun An. Data Classification of Mental Health and Personality Evaluation Based on Network Deep Learning.
# September 2022Mobile Information Systems 2022(3):1-11.
# url: https://www.researchgate.net/publication/363239149_Data_Classification_of_Mental_Health_and_Personality_Evaluation_Based_on_Network_Deep_Learning


# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
from transformers import (
    AutoModelForSequenceClassification,     # –ú–æ–¥–µ–ª—å, –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –¥–ª—è –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –¥–∞–Ω–Ω—ã—Ö.
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    # AutoModelForCausalLM,        # –ú–æ–¥–µ–ª—å, –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –¥–ª—è LLM.
)
from datasets import Dataset
import torch

# ___ –ú–æ–¥—É–ª—å rich –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞ –≤ –∫–æ–Ω—Å–æ–ª–∏: ____________________________________________
from rich import print              # Description: https://github.com/textualize/rich/blob/master/README.ru.md
                                    # Colors: https://rich.readthedocs.io/en/stable/appendix/colors.html
from rich.console import Console
# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

# –ê–∫—Ç–∏–≤–∞—Ü–∏—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ rich –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–π –ø–µ—á–∞—Ç–∏ –≤ –¢–µ—Ä–º–∏–Ω–∞–ª–µ:
console = Console()

# ___ –ò–ù–°–¢–†–£–ö–¶–ò–Ø –ø–æ –ø–µ—á–∞—Ç–∏ ___
# –ü–µ—á–∞—Ç—å —Ü–≤–µ—Ç–Ω–æ–≥–æ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è –≤ –¢–ï–†–ú–ò–ù–ê–õ–ï —Å –ø–æ–º–æ—â—å—é –ø–∞–∫–µ—Ç–∞ rich:
# –ò–õ–ò —Ç–∞–∫:
# rprint(f'[red]{'*' * 60}[/red]\n{query_1[:-1]}:\n {vector_1}')
# –ò–õ–ò —Ç–∞–∫:
# console.print(f'[red]{'*' * 60}[/red]\n{query_1[:-1]}:\n {vector_1}')



# ______  1. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞  _________________________________________
# –ò–º–ø–æ—Ä—Ç –ü–†–ï–î–û–ë–£–ß–ï–ù–ù–û–ô –º–æ–¥–µ–ª–∏:
model_name = "distilbert-base-uncased"      # –û—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è –∏ –º–∞–ª–µ–Ω—å–∫–∞—è –º–æ–¥–µ–ª—å, —á—Ç–æ–±—ã –º–æ–∂–Ω–æ –±—ã–ª–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å –∏ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ.
# –î–∞–ª–µ–µ –º–æ–¥–µ–ª—å –ø—Ä–æ–∫–∏–¥—ã–≤–∞—Ç—Å—è –≤ –º–µ—Ç–æ–¥  .from_pretrained() –¥–ª—è –ü–û–õ–£–ß–ï–ù–ò–Ø –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏ —ç—Ç–æ–π –º–æ–¥–µ–ª–∏:
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)
# –¢–æ–∫–µ–Ω–∞–∑–µ—Ä –Ω—É–∂–µ–Ω –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ –ò–ù–ü–£–¢–ê:
tokenizer = AutoTokenizer.from_pretrained(model_name)   # –Ω—É–∂–µ–Ω –í –°–í–Ø–ó–ö–ï —Å –º–æ–¥–µ–ª—å—é, –ø–æ—Ç–æ–º—É —á—Ç–æ
# –º–æ–¥–µ–ª—å –∏–º–µ–µ—Ç —Å–≤–æ–∏ —Å–ø–æ—Å–æ–±—ã —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —Ç–æ–∫–µ–Ω—ã.

# –î–ª—è –º–æ–¥–µ–ª–µ–π —Ç–∏–ø–∞ DistilBERT –æ–±—ã—á–Ω–æ pad_token –µ—Å—Ç—å, –Ω–æ –∏–Ω–æ–≥–¥–∞ –º–æ–∂–µ—Ç –≤–æ–∑–Ω–∏–∫–∞—Ç—å warning.
# https://chatgpt.com/s/t_68585d8a625881918b9e4c74d14e4102
# –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π:
tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token


# ______  2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞  ___________________________________________________________________
file_path = "dataset_neurotics.txt"
labels = []
texts = []
with open(file_path, "r", encoding="utf-8") as f:
    for line in f:
        line = line.strip()     # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        if not line:
            continue            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        try:
            label, text = line.split(" - ", 1)     # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ ' - '
            labels.append(int(label.strip()))                   # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ª–µ–π–±–ª –≤ int
            texts.append(text.strip())
        except ValueError:
            print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–∞ —Å—Ç—Ä–æ–∫–∞ –∏–∑-–∑–∞ —Ñ–æ—Ä–º–∞—Ç–∞: {line}")


data = {
    "text": texts,
    "label": labels}
dataset = Dataset.from_dict(data)
# Video 10, 40:50 - –∫–∞–∫ –ø–æ–¥–≥—Ä—É–∑–∏—Ç—å –¥—Ä—É–≥–æ–π –¥–∞—Ç–∞—Å–µ—Ç.


# ______  3. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è  ___________________________________________________________________________
def tokenize(example):
    return tokenizer(example["text"], truncation=True, padding="max_length")
# truncation - –æ—á–∏—Å—Ç–∫–∞ –æ—Ç –õ–ò–®–ù–ò–• –ø—Ä–æ–±–µ–ª–æ–≤.

# –ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –≤ tokenize(), –Ω–∞ –í–°–ï –∏–Ω–ø—É—Ç—ã:
tokenized_dataset = dataset.map(tokenize, batched=True)

# üîπ –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏:
train_test = tokenized_dataset.train_test_split(test_size=0.2, seed=42)
train_dataset = train_test["train"]
test_dataset = train_test["test"]


# ______  4. –ê—Ä–≥—É–º–µ–Ω—Ç—ã —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ (–æ–±—É—á–µ–Ω–∏—è)  _______________________________________________________
training_args = TrainingArguments(
    output_dir="./results_hw_10",    # –í—ã—Ö–æ–¥–Ω–∞—è –ø–∞–ø–∫–∞ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
    per_device_train_batch_size=4,          # –î–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–µ–∂–¥—É –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞–º–∏ –∏–ª–∏ –≤–∏–¥–µ–æ–∫–∞—Ä—Ç–∞–º–∏, —Ç–µ –∫–∞–∫–∏—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –±—É–¥–µ—Ç –±–∞—Ç—á –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ–≤–∞–π—Å–∞.
    num_train_epochs=2,                     # –≠–ø–æ—Ö–∞ = 1 —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è: –≤–≤–æ–¥ –∏–Ω–ø—É—Ç–æ–≤ -->
                                            # --> backpropagation - –æ—à–∏–±–∫–∏ –Ω–∞–∑–∞–¥ –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ –í–°–ï–û–í –Ω–∞ –≤—Å–µ—Ö –Ω–µ–π—Ä–æ–Ω–∞—Ö.
    logging_dir="./logs_hw_10",      # –ü–∞–ø–∫–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ª–æ–≥–æ–≤ –æ—Ç –æ–±—É—á–µ–Ω–∏—è.
    logging_steps=10                        # –ö–∞–∫ —á–∞—Å—Ç–æ –±—É–¥–µ –ø—Ä–æ—Ö–æ–¥–∏—Ç—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ.
)

# ______  5. –û–±—É—á–µ–Ω–∏–µ  ______________________________________________________________________________
# –ö–ª–∞—Å—Å, –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π –∑–∞ –æ–±—É—á–µ–Ω–∏–µ:
trainer = Trainer(
    model=model,                        # –ï–º—É –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è: - –º–æ–¥–µ–ª—å
    args=training_args,                 #                 - —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã).
    train_dataset=train_dataset,    # –ü–µ—Ä–µ–¥–∞—á–∞ –ü–û–î–ì–û–¢–û–í–õ–ï–ù–ù–û–ì–û —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞.
    eval_dataset=test_dataset     # ‚¨ÖÔ∏è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–µ
)
trainer.train()

# ______  6. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –Ω–æ–≤–æ–º –ø—Ä–∏–º–µ—Ä–µ  __________________________________________________________
# –î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –î–û–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:
text = "–î–∞, —è –≤–µ—Ä—é –≤ –∞—Å—Ç—Ä–æ–ª–æ–≥–∏—é"      # –í–≤–æ–¥–∏–º —Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–º —Ö–æ—Ç–∏–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –î–û–æ–±—É—á–µ–Ω–Ω–æ –º–æ–¥–µ–ª–∏.
# text = "–ï—Å–ª–∏ –º–Ω–µ –Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è –º–æ—è —Ä–∞–±–æ—Ç–∞ –∏–ª–∏ –º–µ—Å—Ç–æ, –≥–¥–µ —è –∂–∏–≤—É, - —è –Ω–µ —Å—Ç–∞–Ω—É —Ç–µ—Ä–ø–µ—Ç—å: –ø–æ–º–µ–Ω—è—é —Ä–∞–±–æ—Ç—É –∏–ª–∏ –ø–µ—Ä–µ–µ–¥—É –≤ –¥—Ä—É–≥–æ–µ –º–µ—Å—Ç–æ."
inputs = tokenizer(text, return_tensors="pt")       # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –Ω–æ–≤–æ–≥–æ –∏–Ω–ø—É—Ç–∞.

# ______  7. –ü–µ—Ä–µ–º–µ—â–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ —Ç–æ –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, –≥–¥–µ –º–æ–¥–µ–ª—å  _______________________________
inputs = {k: v.to(model.device) for k, v in inputs.items()}     # \\\\\\  NB! - –¥–æ—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–¥–∞ //////

# ______  8. –î–µ–ª–∞–µ–º –≤—ã–≤–æ–¥  ____________________________________________________________________________
model.eval()
with torch.no_grad():
    output = model(**inputs)
    pred = output.logits.argmax(dim=-1).item()      # .logits - —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Ç 0 –¥–æ 1.
    label_map = {
        0: "–ß–µ–ª–æ–≤–µ–∫ —Å –Ω–æ—Ä–º–∞–ª—å–Ω—ã–º–∏ –ª–∏—á–Ω—ã–º–∏ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏. –£–≤–µ—Ä–µ–Ω –≤ —Å–µ–±–µ. "
           "–ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–µ–±—è —Ç–∞–∫–∏–º –∫–∞–∫–æ–π –µ—Å—Ç—å, –æ–¥–Ω–∞–∫–æ –ø—Ä–∏—Å–ª—É—à–∏–≤–∞–µ—Ç—Å—è –∫ —Å–≤–æ–∏–º –æ—â—É—â–µ–Ω–∏—è–º –∏ —á—É–≤—Å—Ç–≤–∞–º",
        1: "–í —Ü–µ–ª–æ–º —á–µ–ª–æ–≤–µ–∫ —Å–ø–æ—Å–æ–±–µ–Ω –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —Å–≤–æ–∏ –ª–∏—á–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –∏ –æ–±–æ–∑–Ω–∞—á–∞—Ç—å –∏—Ö. –ï—Å—Ç—å –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è.",
        2: "–ï—Å—Ç—å –Ω–µ–∫—Ä–∏—Ç–∏—á–Ω—ã–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è, –≤ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–µ–ø–µ–Ω–∏ —Å–Ω–∏–∂–∞—é—â–∏–µ –∫–∞—á–µ—Å—Ç–≤–æ –∂–∏–∑–Ω–∏ —Å–∞–º–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞ –∏ –µ–≥–æ –æ–∫—Ä—É–∂–∞—é—â–∏—Ö. –ì–æ—Ç–æ–≤ –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –ø—Å–∏—Ö–æ–ª–æ–≥—É.",
        3: "–ß–µ–ª–æ–≤–µ–∫ –æ—Å–æ–∑–Ω–∞–µ—Ç –∫–∞–∫–∏–µ-—Ç–æ –ø—Ä–æ–±–ª–µ–º—ã —Å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ª–∏—á–Ω—ã–º–∏ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏, –Ω–æ –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç –∫–∞–∫ –∏—Ö —Ä–µ—à–∞—Ç—å.",
        4: "–ù–µ–≤—Ä–æ—Ç–∏–∫ —Å –¥–æ–≤–æ–ª—å–Ω–æ –≤—ã—Å–æ–∫–æ–π —Å—Ç–µ–ø–µ–Ω—å—é —Ä–∞—Å—Å—Ç—Ä–æ–π—Å—Ç–≤–∞. –î–∞–∂–µ –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç, —á—Ç–æ —É –Ω–µ–≥–æ –ø—Ä–æ–±–ª–µ–º—ã."
    }
    print(f'{'===  –†–ï–ó–£–õ–¨–¢–ê–¢  ':=<{l}}')
    print(label_map.get(pred, "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∫–ª–∞—Å—Å"))
