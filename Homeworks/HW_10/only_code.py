from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
)
from datasets import Dataset
import torch

# ______  1. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
model_name = "distilbert-base-uncased"
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token

# ______  2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
file_path = "dataset_neurotics.txt"
labels = []
texts = []
with open(file_path, "r", encoding="utf-8") as f:
    for line in f:
        line = line.strip()
        if not line:
            continue
        try:
            label, text = line.split(" - ", 1)
            labels.append(int(label.strip()))
            texts.append(text.strip())
        except ValueError:
            print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–∞ —Å—Ç—Ä–æ–∫–∞ –∏–∑-–∑–∞ —Ñ–æ—Ä–º–∞—Ç–∞: {line}")


data = {
    "text": texts,
    "label": labels}
dataset = Dataset.from_dict(data)

# ______  3. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
def tokenize(example):
    return tokenizer(example["text"], truncation=True, padding="max_length")

tokenized_dataset = dataset.map(tokenize, batched=True)

# üîπ –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏:
train_test = tokenized_dataset.train_test_split(test_size=0.2, seed=42)
train_dataset = train_test["train"]
test_dataset = train_test["test"]


# ______  4. –ê—Ä–≥—É–º–µ–Ω—Ç—ã —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ (–æ–±—É—á–µ–Ω–∏—è)
training_args = TrainingArguments(
    output_dir="./results_hw_10",
    per_device_train_batch_size=4,
    num_train_epochs=2,
    logging_dir="./logs_hw_10",
    logging_steps=10
)

# ______  5. –û–±—É—á–µ–Ω–∏–µ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset     # ‚¨ÖÔ∏è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–µ
)
trainer.train()

# ______  6. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –Ω–æ–≤–æ–º –ø—Ä–∏–º–µ—Ä–µ
text = "–î–∞, —è –≤–µ—Ä—é –≤ –∞—Å—Ç—Ä–æ–ª–æ–≥–∏—é"
inputs = tokenizer(text, return_tensors="pt")

# ______  7. –ü–µ—Ä–µ–º–µ—â–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ —Ç–æ –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, –≥–¥–µ –º–æ–¥–µ–ª—å
inputs = {k: v.to(model.device) for k, v in inputs.items()}

# ______  8. –î–µ–ª–∞–µ–º –≤—ã–≤–æ–¥
model.eval()
with torch.no_grad():
    output = model(**inputs)
    pred = output.logits.argmax(dim=-1).item()
    label_map = {
        0: "–ß–µ–ª–æ–≤–µ–∫ —Å –Ω–æ—Ä–º–∞–ª—å–Ω—ã–º–∏ –ª–∏—á–Ω—ã–º–∏ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏.",
        1: "–í —Ü–µ–ª–æ–º —á–µ–ª–æ–≤–µ–∫ —Å–ø–æ—Å–æ–±–µ–Ω –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —Å–≤–æ–∏ –ª–∏—á–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –∏ –æ–±–æ–∑–Ω–∞—á–∞—Ç—å –∏—Ö. –ï—Å—Ç—å –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è.",
        2: "–ï—Å—Ç—å –Ω–µ–∫—Ä–∏—Ç–∏—á–Ω—ã–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è, –≤ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–µ–ø–µ–Ω–∏ —Å–Ω–∏–∂–∞—é—â–∏–µ –∫–∞—á–µ—Å—Ç–≤–æ –∂–∏–∑–Ω–∏ —Å–∞–º–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞ –∏ –µ–≥–æ –æ–∫—Ä—É–∂–∞—é—â–∏—Ö. –ì–æ—Ç–æ–≤ –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –ø—Å–∏—Ö–æ–ª–æ–≥—É.",
        3: "–ß–µ–ª–æ–≤–µ–∫ –æ—Å–æ–∑–Ω–∞–µ—Ç –∫–∞–∫–∏–µ-—Ç–æ –ø—Ä–æ–±–ª–µ–º—ã —Å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ª–∏—á–Ω—ã–º–∏ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏, –Ω–æ –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç –∫–∞–∫ –∏—Ö —Ä–µ—à–∞—Ç—å.",
        4: "–ù–µ–≤—Ä–æ—Ç–∏–∫ —Å –¥–æ–≤–æ–ª—å–Ω–æ –≤—ã—Å–æ–∫–æ–π —Å—Ç–µ–ø–µ–Ω—å—é —Ä–∞—Å—Å—Ç—Ä–æ–π—Å—Ç–≤–∞. –î–∞–∂–µ –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç, —á—Ç–æ —É –Ω–µ–≥–æ –ø—Ä–æ–±–ª–µ–º—ã."
    }
    print(label_map.get(pred, "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∫–ª–∞—Å—Å"))
