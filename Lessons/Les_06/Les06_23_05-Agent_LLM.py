# Müller Alexader
# \033[0;__;__m \033[m   or   \033[1;__;__m \033[m
# print('#' * 115)      # Для разделения блоков на листе с кодом:
""" ################################################################################################################
 23.05.25
 AI  &  Python 6: Агентный подход в разработке с использованием LLM.
 GitHub Course: https://github.com/viacheslav-bandylo/llm-course/tree/main
 ################################################################################################################### """

# ------------------------ SHORTCUTS ------------------------
# Ctrl + W - выделить текущий блок. если нажимать это сочетание дальше, то будут выделяться родительские блоки.
# Ctrl+X - Удаление всей строки. Кстати, команда копирования Ctrl+C без выделения также работает для всей строки.
# Ctrl+Akt+L / Alt+Enter - Привести код к принятым стандартам (для Python - PEP8).
# Ctrl+R — Изменить название класса/функции и т.п. по всему проекту.
# Ctrl+Shift + F - Найти по всем файлам.
# Shift + F6 - заменить имя элемента во всех частях во всех файлах.
# -----------------------------------------------------------

print('.' * 80)


"""" %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%_________     Пример простого агента    ________%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                                                    с помощью LangChain                                          """
# Смотри Lect 7, slide 27.

""" ___ ЗАДАНИЕ ___ """
# ___ Создание агента-поисковика, который сможет отвечать на вопросы, используя поисковик Tavily:
# https://app.tavily.com/home

# Сначала установить langgraph в терминале:
# pip install langgraph

# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# Импортируем необходимые классы и функции из сторонних библиотек:
from langchain_google_genai import ChatGoogleGenerativeAI       # Класс для работы с моделью генеративного ИИ от Google (в данном случае Gemini).
from langchain_community.tools.tavily_search import TavilySearchResults  # Инструмент для выполнения поисковых запросов через Tavily.
from langchain_core.messages import HumanMessage                # Класс для создания сообщений, имитирующих ввод от человека.
from langgraph.checkpoint.memory import MemorySaver             # Модуль для сохранения состояния (памяти) диалога.
from langgraph.prebuilt import create_react_agent               # Функция для создания агента, использующего паттерн "ReAct" (Reasoning + Acting).
from dotenv import load_dotenv                                  # Функция для загрузки переменных окружения из файла .env.
import os                   # Модуль для работы с операционной системой, в частности, для работы с переменными окружения.
# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

# Загружаем переменные окружения из файла .env:
load_dotenv()

# Получаем API-ключ для модели Gemini и Tavily из переменных окружения:
# os.environ["GOOGLE_API_KEY"] = os.getenv("GEMINI_API_KEY")    # 1 - Можно так к Гуглу обращаться.
api_key = os.getenv("GEMINI_API_KEY")                           # 2 - А можно и ТАК - через переменную ОКРУЖЕНИЯ в PyCharm.
os.environ["TAVILY_API_KEY"] = os.getenv("TAVILY_API_KEY")

# Создаём экземпляр генеративной модели ИИ от Google, используя модель "gemini-2.0-flash":
# llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash")                    # 1 - Можно так к Гуглу обращаться.
llm = ChatGoogleGenerativeAI(api_key=api_key, model="gemini-2.0-flash")     # 2 - А можно и ТАК - через переменную ОКРУЖЕНИЯ в PyCharm.

# Инициализируем объект для сохранения состояния диалога (память агента):
memory = MemorySaver()

# Создаём объект инструмента для поиска, ограничивая число результатов до 2:
search = TavilySearchResults(max_results=2)

# Собираем все инструменты в список. Здесь используется только инструмент поиска:
tools = [search]

# Создаём агента, который будет обрабатывать запросы, для этого передаём ему генеративную модель,
# список инструментов и объект для сохранения памяти:
agent_executor = create_react_agent(llm, tools, checkpointer=memory)

# Создаём конфигурационный словарь с настройками, например, идентификатором потока:
config = {"configurable": {"thread_id": "abc123"}}

# --- 1-ы й цикл: отправляем сообщение "hi im bob! and i live in berlin" агенту и выводим
# промежуточные шаги выполнения:
for step in agent_executor.stream(
    {"messages": [HumanMessage(content="Hi im Bob! And i live in Berlin")]},
    config,
    stream_mode="values",
):
    # Выводим последнее сообщение из текущего шага в красивом формате:
    step["messages"][-1].pretty_print()     # .pretty_print() - из библиотеки LangChain.

# --- 2-ой цикл: отправляем сообщение "whats the weather where I live?" агенту.
# Агент использует накопленную память и, возможно, инструмент поиска, чтобы сформировать ответ:
for step in agent_executor.stream(
    {"messages": [HumanMessage(content="Whats the weather where I live?")]},
    config,
    stream_mode="values",
):
    # Аналогично, выводим последнее сообщение из каждого шага:
    step["messages"][-1].pretty_print()




""" ___________________________________  Review of previously covered material  ___________________________________ """
"""" %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%___________   ---   __________%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% """
""" __________ --- __________ """
""" __________ --- __________ """
#       ●
# ___ EXAMPLE __________________________________________________
# ___ END of Example __________________________________________________


""" ______  Task 1  ______________________________________________________________________________________________ """
#


